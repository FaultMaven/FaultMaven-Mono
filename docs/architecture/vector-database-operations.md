# Vector Database Operations & Data Flow Guide

**Document Type:** Operational Guide
**Version:** 1.0
**Last Updated:** 2025-10-21
**Status:** ğŸŸ¢ **PRODUCTION READY**
**Related Docs:** [knowledge-base-architecture.md](./knowledge-base-architecture.md), [qa-tools-design.md](./qa-tools-design.md)

---

## Overview

This document provides comprehensive operational details for FaultMaven's vector database system. While [knowledge-base-architecture.md](./knowledge-base-architecture.md) explains **what** the three KB systems are and **why** they exist, this document explains **how** they work operationally.

**Scope**:
- Document ingestion pipelines (how data gets into vector DB)
- Query execution flows (how searches work)
- Collection lifecycle management (creation, usage, cleanup)
- API endpoint specifications
- Operational procedures (admin tasks, monitoring, backup/restore)

**Out of Scope**:
- Conceptual architecture (see knowledge-base-architecture.md)
- Q&A tool design and prompt engineering (see qa-tools-design.md)
- Strategy Pattern implementation details (see knowledge-base-architecture.md)

---

## Physical Architecture

### ChromaDB Deployment

**Single Instance, Multiple Collections Pattern**:

```
ChromaDB Instance
â”œâ”€â”€ Host: chromadb.faultmaven.local
â”œâ”€â”€ Port: 30080 (NodePort in K8s)
â”œâ”€â”€ Auth: Token-based (optional)
â””â”€â”€ Collections:
    â”œâ”€â”€ faultmaven_kb (Global KB)
    â”œâ”€â”€ user_alice_kb (Alice's User KB)
    â”œâ”€â”€ user_bob_kb (Bob's User KB)
    â”œâ”€â”€ case_abc123 (Case Evidence)
    â””â”€â”€ case_xyz789 (Case Evidence)
```

**Design Decision**: Single instance with multiple collections (vs separate instances)
- **Pros**: Simpler deployment, resource efficiency, easier backup
- **Cons**: Shared resource pool (mitigated by collection-level isolation)
- **Scaling**: Collections are independently queryable; ChromaDB handles isolation

### Embedding Model

**Current**: BGE-M3 (BAAI/bge-m3)
- **Dimensions**: 1024
- **Max Sequence Length**: 8192 tokens
- **Language Support**: Multilingual (100+ languages)
- **Model Size**: ~2.3GB
- **Loading**: Cached in memory via `model_cache.get_bge_m3_model()`

**Location**: Loaded in-process (not external service)
- KnowledgeIngester: For global KB document ingestion
- PreprocessingService: For case evidence chunking
- Q&A Tools: Generate query embeddings on the fly

### Connection Management

**Three Client Patterns**:

1. **ChromaDBVectorStore** (Global KB):
```python
# Singleton pattern, connects to single collection
client = chromadb.HttpClient(host="chromadb.faultmaven.local", port=30080)
collection = client.get_or_create_collection("faultmaven_kb")
```

2. **CaseVectorStore** (Case Evidence):
```python
# Multi-collection pattern, dynamic collection per case
client = chromadb.HttpClient(host="chromadb.faultmaven.local", port=30080)
collection = client.get_or_create_collection(f"case_{case_id}")
```

3. **KnowledgeIngester** (Admin Operations):
```python
# Direct client for batch operations
client = chromadb.HttpClient(host="chromadb.faultmaven.local", port=30080)
# Can create/manage any collection
```

---

## Collection Lifecycle Management

### 1. Global KB (faultmaven_kb)

**Lifecycle**: Permanent (never deleted)

**Creation**:
- **When**: Pre-initialized during system deployment
- **How**: KnowledgeIngester creates on first run
- **Trigger**: Manual admin operation or automated deployment script

**Creation Code**:
```python
# In KnowledgeIngester.__init__()
self.collection = self.chroma_client.get_or_create_collection(
    name="faultmaven_kb",
    metadata={"description": "FaultMaven Knowledge Base"}
)
```

**Metadata Schema**:
```python
{
    "document_id": "uuid-v4",
    "title": "Document title",
    "document_type": "troubleshooting_guide | error_reference | architecture_doc",
    "tags": ["tag1", "tag2"],
    "source_url": "https://docs.example.com/page",
    "ingested_at": "2025-10-21T10:30:00Z",
    "chunk_index": 0,  # Which chunk of the original document
    "total_chunks": 5   # How many chunks total
}
```

**Population**:
- **Admin-initiated**: Via `/api/v1/knowledge/documents` POST endpoint
- **Batch import**: Via KnowledgeIngester.ingest_document()
- **Frequency**: Infrequent (documentation updates, new guides added)

**Cleanup**: None (permanent storage)

---

### 2. User KB (user_{user_id}_kb)

**Lifecycle**: Permanent per user (deleted only if user account deleted)

**Creation**:
- **When**: Lazy creation on first document upload by user
- **How**: get_or_create_collection() on first User KB document POST
- **Trigger**: User uploads personal runbook/procedure via UI

**Creation Code**:
```python
# In UserKBVectorStore._get_or_create_collection()
collection_name = f"user_{user_id}_kb"
metadata = {
    "user_id": user_id,
    "created_at": datetime.now(timezone.utc).isoformat(),
    "type": "user_knowledge_base"
}
collection = self.client.get_or_create_collection(
    name=collection_name,
    metadata=metadata
)
```

**Metadata Schema**:
```python
{
    "document_id": "uuid-v4",
    "user_id": "user_alice",
    "title": "Database Timeout Troubleshooting",  # From upload
    "filename": "db_timeout.md",
    "category": "database",  # User-specified category
    "tags": "postgresql,timeout,performance",  # Comma-separated
    "description": "Runbook for database timeout issues",
    "data_type": "UNSTRUCTURED_TEXT",  # From preprocessing
    "file_size": 15360,
    "uploaded_at": "2025-10-22T10:30:00Z",
    "chunk_index": 0,
    "total_chunks": 3
}
```

**Population**:
- **User-initiated**: Via API endpoint `POST /api/v1/users/{user_id}/kb/documents`
- **Frequency**: Occasional (user documents procedures as they learn)
- **Processing**: Full preprocessing pipeline (classify â†’ extract â†’ sanitize â†’ embed â†’ store)

**Cleanup**:
- **Trigger**: User account deletion (GDPR compliance)
- **Method**: `UserKBVectorStore.delete_user_collection(user_id)`
- **Timing**: Synchronous with account deletion

**Implementation Status**: âœ… **FULLY IMPLEMENTED** (2025-10-22)

---

### 3. Case Evidence Store (case_{case_id})

**Lifecycle**: Ephemeral (tied to case lifecycle)

**Creation**:
- **When**: Lazy creation on first file upload to case
- **How**: get_or_create_collection() in CaseVectorStore.add_documents()
- **Trigger**: User uploads file via chat UI (`POST /api/v1/cases/{case_id}/data`)

**Creation Code**:
```python
# In CaseVectorStore._get_or_create_collection()
collection_name = f"case_{case_id}"
metadata = {
    "case_id": case_id,
    "created_at": datetime.now(timezone.utc).isoformat()
}
collection = self.client.get_or_create_collection(
    name=collection_name,
    metadata=metadata
)
```

**Metadata Schema**:
```python
{
    "case_id": "abc123",
    "document_id": "uuid-v4",
    "filename": "app.log",
    "file_type": "logs_and_errors | metrics | config | code | text",
    "uploaded_at": "2025-10-21T10:30:00Z",
    "file_size_bytes": 524288,
    "chunk_index": 0,
    "total_chunks": 12,
    # Type-specific metadata
    "line_number": 42,  # For logs
    "timestamp": "2025-10-21T10:25:15Z",  # For logs/metrics
    "severity": "ERROR"  # For logs
}
```

**Population**:
- **User-initiated**: File upload during active investigation
- **Frequency**: Frequent (multiple uploads per case)
- **Batch processing**: Preprocessing pipeline (classify â†’ extract â†’ chunk â†’ embed â†’ store)

**Cleanup**:
- **Trigger**: Case status change to CLOSED or ARCHIVED
- **Method**: `CaseVectorStore.delete_case_collection(case_id)`
- **Timing**: Synchronous with case closure
- **Implementation**: âœ… **IMPLEMENTED**

**Cleanup Code**:
```python
# In CaseService.close_case() or archive_case()
async def close_case(self, case_id: str):
    # ... update case status ...

    # Delete case evidence collection
    await self.case_vector_store.delete_case_collection(case_id)

    # Collection "case_{case_id}" is now deleted from ChromaDB
```

---

## Document Ingestion Pipeline

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ File Upload  â”‚
â”‚  (user/admin)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Classificationâ”‚  â† Determine file type (logs, metrics, config, etc.)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Extraction    â”‚  â† Extract relevant content (crime scenes, anomalies, etc.)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Chunking      â”‚  â† Split into embeddable chunks (â‰¤8K tokens)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† Large docs (>8K) use ChunkingService (map-reduce)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Sanitization  â”‚  â† Redact PII/secrets
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Embedding     â”‚  â† Generate vectors with BGE-M3
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Storage       â”‚  â† Add to ChromaDB collection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Steps by KB Type

#### Global KB Ingestion (Admin Upload)

**Endpoint**: `POST /api/v1/knowledge/documents`

**Request**:
```json
{
  "file": "<multipart file>",
  "title": "PostgreSQL Timeout Troubleshooting Guide",
  "document_type": "troubleshooting_guide",
  "tags": ["database", "postgresql", "timeout"],
  "source_url": "https://wiki.example.com/postgres-timeouts"
}
```

**Processing**:
1. **Upload**: File saved temporarily to `/tmp/`
2. **Extraction**: Parse document (PDF/DOCX/MD/TXT)
3. **Chunking**: Split into chunks (512 tokens, 128 overlap)
4. **Sanitization**: Redact any PII (unlikely in admin docs, but defensive)
5. **Embedding**: Generate BGE-M3 embeddings (batch of chunks)
6. **Storage**: Add to `faultmaven_kb` collection
7. **Cleanup**: Delete temp file

**Code Path**:
```
POST /api/v1/knowledge/documents
  â†’ KnowledgeIngester.ingest_document(file_path, title, ...)
    â†’ _extract_text_from_file(file_path)
    â†’ _chunk_text(text, chunk_size=512)
    â†’ _sanitize_chunks(chunks)
    â†’ _generate_embeddings(chunks)
    â†’ collection.add(ids, embeddings, documents, metadatas)
```

**Performance**:
- Small doc (5 pages): ~2-3 seconds
- Large doc (50 pages): ~15-20 seconds
- **Async**: Runs as background job with status polling

---

#### User KB Ingestion (User Upload)

**Endpoint**: `POST /api/v1/users/{user_id}/kb/documents`

**Request**:
```bash
curl -X POST http://api.faultmaven.local/api/v1/users/alice/kb/documents \
  -H "Authorization: Bearer $USER_TOKEN" \
  -F "file=@my_runbook.md" \
  -F "title=Database Timeout Troubleshooting" \
  -F "category=database" \
  -F "tags=postgresql,timeout,performance" \
  -F "description=Runbook for handling database timeouts"
```

**Processing**:
1. **Upload**: File received via multipart form data
2. **Preprocessing**: Full 4-step pipeline (classify â†’ extract â†’ sanitize â†’ chunk)
3. **Embedding**: BGE-M3 embeddings generated for each chunk
4. **Storage**: Add to `user_{user_id}_kb` collection with metadata
5. **Response**: 201 Created with document_id and preprocessing metrics

**Code Path**:
```
POST /api/v1/users/{user_id}/kb/documents
  â†’ user_kb.upload_user_kb_document()
    â†’ preprocessing_service.preprocess(filename, content)
    â†’ user_kb_vector_store.add_documents(user_id, documents)
      â†’ _generate_embeddings(chunks)
      â†’ collection.add(ids, embeddings, documents, metadatas)
```

**Access Control**:
- Verify `current_user.user_id == user_id` (users can only upload to their own KB)
- Returns 403 if trying to upload to another user's KB

**Performance**:
- Small doc (5 pages): ~2-3 seconds
- Large doc (50 pages): ~15-20 seconds (with ChunkingService map-reduce for >8K tokens)

**Implementation Status**: âœ… **FULLY IMPLEMENTED** (2025-10-22)

---

#### Case Evidence Ingestion (User Upload During Investigation)

**Endpoint**: `POST /api/v1/cases/{case_id}/data`

**Request**:
```json
{
  "file": "<multipart file>",
  "session_id": "session-uuid",
  "description": "Application log from production server"  // Optional
}
```

**Processing**:
1. **Classification**: Determine file type (logs, metrics, config, code, text, visual)
2. **Extraction**: Type-specific extraction (crime scenes for logs, anomalies for metrics, etc.)
3. **Chunking**: Smart chunking based on type
   - **Logs**: Crime scene extraction (keep related lines together)
   - **Metrics**: Statistical summaries per time window
   - **Config**: Hierarchical structure preservation
   - **Code**: Function/class boundaries
   - **Text**: Semantic chunking (512 tokens, 128 overlap)
   - **Large Documents** (>8K tokens): ChunkingService map-reduce pattern (âœ… IMPLEMENTED)
     - 4K token chunks with 200 token overlap
     - Parallel MAP phase (up to 5 concurrent LLM calls)
     - REDUCE phase synthesizes chunk summaries
     - 80% information retention vs 25% with truncation
     - See `faultmaven/services/preprocessing/chunking_service.py`
4. **Sanitization**: PII/secret redaction (CRITICAL for user uploads)
5. **Embedding**: BGE-M3 embeddings per chunk
6. **Storage**: Add to `case_{case_id}` collection

**Code Path**:
```
POST /api/v1/cases/{case_id}/data
  â†’ PreprocessingService.preprocess(file, case_id)
    â†’ DataClassifier.classify(file)
    â†’ {Type}Extractor.extract(file)  # Type-specific
    â†’ ChunkingService.chunk(extracted)
    â†’ DataSanitizer.sanitize(chunks)
    â†’ CaseVectorStore.add_documents(case_id, chunks)
      â†’ _generate_embeddings(chunks)
      â†’ collection.add(ids, embeddings, documents, metadatas)
```

**Performance**:
- Small log (100 KB): ~1-2 seconds
- Large log (10 MB): ~5-10 seconds
- **Async**: Background processing with immediate acknowledgment

**Type-Specific Chunking Strategies**:

| File Type | Chunking Strategy | Chunk Size | Metadata Preserved |
|-----------|-------------------|------------|-------------------|
| Logs | Crime scene extraction | Variable (burst-based) | line_number, timestamp, severity |
| Metrics | Time window aggregation | Fixed (per window) | timestamp, metric_name, value |
| Config | Hierarchical sections | Variable (section-based) | section_path, key |
| Code | Function/class boundaries | Variable (AST-based) | function_name, class_name, line_range |
| Text | Semantic chunking | 512 tokens, 128 overlap | chunk_index, total_chunks |

---

## Query Operations

### Query Execution Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Question    â”‚
â”‚ "What errors in â”‚
â”‚  the log?"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Tool Selectionâ”‚  â† Main agent selects answer_from_case_evidence
â”‚   (LLM Function  â”‚     (or answer_from_user_kb, answer_from_global_kb)
â”‚    Calling)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Query         â”‚  â† Generate embedding for "What errors in the log?"
â”‚   Embedding      â”‚     using BGE-M3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Vector Search â”‚  â† ChromaDB cosine similarity search
â”‚   (ChromaDB)     â”‚     in case_{case_id} collection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Top-K         â”‚  â† Retrieve top 5 most similar chunks
â”‚   Selection      â”‚     with similarity scores
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Context       â”‚  â† Build context string with metadata
â”‚   Building       â”‚     (line numbers, timestamps, etc.)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. LLM Synthesis â”‚  â† GPT-4-mini synthesizes answer from chunks
â”‚   (GPT-4-mini)   â”‚     using KB-specific system prompt
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Formatted     â”‚  â† Return answer with citations
â”‚   Response       â”‚     "347 ERROR entries at lines 42-89, ..."
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Search API Specifications

#### Global KB Search

**Endpoint**: `POST /api/v1/knowledge/search`

**Request**:
```json
{
  "query": "How to diagnose PostgreSQL connection timeouts?",
  "k": 5,  // Number of results
  "filters": {
    "document_type": "troubleshooting_guide",
    "tags": ["database", "postgresql"]
  }
}
```

**Response**:
```json
{
  "results": [
    {
      "document_id": "uuid-1",
      "title": "PostgreSQL Timeout Troubleshooting",
      "content": "Connection timeouts in PostgreSQL can occur when...",
      "score": 0.89,
      "metadata": {
        "document_type": "troubleshooting_guide",
        "tags": ["database", "postgresql", "timeout"],
        "chunk_index": 2,
        "total_chunks": 5
      }
    },
    // ... more results
  ],
  "total_results": 5,
  "query_time_ms": 45
}
```

**Implementation**: âœ… **IMPLEMENTED** (knowledge.py routes)

---

#### User KB Search

**Via Q&A Tool** (Primary method):

**Tool Call**:
```python
# Main agent calls tool during troubleshooting
answer = await answer_from_user_kb._arun(
    user_id="alice",
    question="My rollback procedure for database changes",
    k=5
)
```

**Internal Flow**:
```python
# In AnswerFromUserKB._arun()
collection_name = kb_config.get_collection_name(user_id)  # "user_alice_kb"
chunks = await vector_store.search(
    collection_name=collection_name,
    query=question,
    k=k
)
# Synthesizes answer from user's documented procedures
```

**Via Direct API** (Management/listing):

**Endpoint**: `GET /api/v1/users/{user_id}/kb/documents`

**Request**:
```bash
curl "http://api.faultmaven.local/api/v1/users/alice/kb/documents?category=database&limit=10" \
  -H "Authorization: Bearer $USER_TOKEN"
```

**Response**:
```json
{
  "status": "success",
  "user_id": "alice",
  "total_count": 25,
  "returned_count": 10,
  "offset": 0,
  "documents": [
    {
      "id": "doc-uuid-1",
      "metadata": {
        "title": "Database Timeout Troubleshooting",
        "category": "database",
        "filename": "db_timeout.md",
        "uploaded_at": "2025-10-22T10:30:00Z"
      }
    }
  ]
}
```

**Access Control**:
- Verify `current_user.user_id == user_id`
- Return 403 if user tries to access another user's KB

**Implementation**: âœ… **FULLY IMPLEMENTED** (2025-10-22)

---

#### Case Evidence Search

**Endpoint**: Via Q&A Tool (not direct HTTP endpoint)

**Tool Call**:
```python
# Main agent calls tool
answer = await answer_from_case_evidence._arun(
    case_id="abc123",
    question="What errors are in the log?",
    k=5
)
```

**Internal Flow**:
```python
# In DocumentQATool.answer_question()
collection_name = kb_config.get_collection_name(case_id)  # "case_abc123"
chunks = await vector_store.search(
    collection_name=collection_name,
    query=question,
    k=k
)
# Returns top 5 chunks with metadata
```

**Response**: Synthesized answer string with citations

**Access Control**:
- Implicit via case access (user must have access to case)
- Enforced at API layer (case query endpoint verifies case ownership)

**Implementation**: âœ… **IMPLEMENTED** (CaseVectorStore + DocumentQATool)

---

### Query Performance Characteristics

| Operation | Typical Latency | P95 Latency | Notes |
|-----------|----------------|-------------|-------|
| Query embedding | 50-100ms | 150ms | BGE-M3 in-process |
| Vector search (k=5, <1000 docs) | 10-30ms | 50ms | ChromaDB native |
| Vector search (k=5, 10K docs) | 30-80ms | 120ms | Collection size dependent |
| LLM synthesis | 500-1500ms | 2500ms | GPT-4-mini, depends on chunk size |
| **Total (end-to-end)** | **600-1700ms** | **2800ms** | Dominated by LLM call |

**Optimization Opportunities**:
- **Caching**: Cache query embeddings for repeated questions
- **Batch**: Batch multiple queries to same collection
- **Precompute**: Pre-generate summaries for common queries

---

## Update & Delete Operations

### Document Updates

**Use Case**: Admin updates Global KB document (e.g., correcting outdated info)

**Endpoint**: `PUT /api/v1/knowledge/documents/{document_id}`

**Request**:
```json
{
  "file": "<new version of document>",
  "title": "Updated PostgreSQL Timeout Guide v2"
}
```

**Process**:
1. **Delete old chunks**: Remove all chunks with `document_id` from collection
2. **Re-ingest**: Run full ingestion pipeline on new version
3. **Preserve metadata**: Keep document_id, but update `last_modified` timestamp

**Code**:
```python
# Delete old chunks
collection.delete(where={"document_id": document_id})

# Re-ingest new version (same document_id)
await ingester.ingest_document(
    new_file_path,
    document_id=document_id,  # Reuse same ID
    ...
)
```

**Consistency**: No versioning - hard replacement
- **Pros**: Simple, no storage overhead
- **Cons**: No rollback capability (mitigated by backup/restore)

**Implementation**: âœ… **IMPLEMENTED** (knowledge.py routes)

---

### Document Deletion

**Use Case**: Remove outdated/incorrect document from Global KB

**Endpoint**: `DELETE /api/v1/knowledge/documents/{document_id}`

**Process**:
1. **Hard delete**: Remove all chunks with `document_id` from collection
2. **No soft delete**: Permanent removal (no recycle bin)

**Code**:
```python
# Delete all chunks for this document
collection.delete(where={"document_id": document_id})
```

**Batch Delete**:
```python
# Delete multiple documents at once
collection.delete(where={"document_id": {"$in": [id1, id2, id3]}})
```

**Implementation**: âœ… **IMPLEMENTED** (knowledge.py routes)

---

### Case Collection Cleanup

**Trigger**: Case status â†’ CLOSED or ARCHIVED

**Method**: `CaseVectorStore.delete_case_collection(case_id)`

**Process**:
1. **Delete entire collection**: `client.delete_collection(f"case_{case_id}")`
2. **No partial delete**: All evidence for case removed at once
3. **Synchronous**: Blocking operation (typically <1 second)

**Code**:
```python
async def delete_case_collection(self, case_id: str) -> None:
    collection_name = self._get_collection_name(case_id)
    try:
        self.client.delete_collection(name=collection_name)
        self.logger.info(f"Deleted case collection: {collection_name}")
    except Exception as e:
        self.logger.error(f"Failed to delete collection {collection_name}: {e}")
        raise
```

**Timing**: Called immediately on case closure (no delayed cleanup)

**Implementation**: âœ… **IMPLEMENTED** (CaseVectorStore)

---

## Operational Procedures

### Adding Documents to Global KB (Admin Workflow)

**Via API** (Recommended):
```bash
# 1. Upload document
curl -X POST http://api.faultmaven.local/api/v1/knowledge/documents \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -F "file=@troubleshooting-guide.pdf" \
  -F "title=PostgreSQL Timeout Troubleshooting Guide" \
  -F "document_type=troubleshooting_guide" \
  -F "tags=database,postgresql,timeout"

# 2. Check ingestion status
# Response includes job_id
curl http://api.faultmaven.local/api/v1/knowledge/jobs/{job_id}

# 3. Verify document appears in search
curl -X POST http://api.faultmaven.local/api/v1/knowledge/search \
  -H "Content-Type: application/json" \
  -d '{"query": "postgres timeout", "k": 5}'
```

**Via Python Script** (Batch Operations):
```python
from faultmaven.core.knowledge.ingestion import KnowledgeIngester

ingester = KnowledgeIngester()

# Ingest multiple documents
documents = [
    ("docs/postgres-timeout.md", "PostgreSQL Timeout Guide", "troubleshooting_guide"),
    ("docs/mysql-locks.md", "MySQL Lock Troubleshooting", "troubleshooting_guide"),
    # ... more documents
]

for file_path, title, doc_type in documents:
    doc_id = await ingester.ingest_document(
        file_path=file_path,
        title=title,
        document_type=doc_type
    )
    print(f"Ingested: {title} (ID: {doc_id})")
```

---

### Backup & Restore Collections

**Backup Global KB**:
```python
import chromadb

client = chromadb.HttpClient(host="chromadb.faultmaven.local", port=30080)
collection = client.get_collection("faultmaven_kb")

# Get all documents
results = collection.get(include=["documents", "metadatas", "embeddings"])

# Save to file
import pickle
with open("faultmaven_kb_backup.pkl", "wb") as f:
    pickle.dump(results, f)
```

**Restore Global KB**:
```python
# Load backup
with open("faultmaven_kb_backup.pkl", "rb") as f:
    backup = pickle.load(f)

# Recreate collection (deletes existing!)
client.delete_collection("faultmaven_kb")
collection = client.create_collection("faultmaven_kb")

# Add all documents back
collection.add(
    ids=backup["ids"],
    documents=backup["documents"],
    metadatas=backup["metadatas"],
    embeddings=backup["embeddings"]
)
```

**Frequency**: Weekly automated backups recommended

---

### Reindexing Collections (Embedding Model Change)

**Use Case**: Switching from BGE-M3 to a newer embedding model

**Process**:
1. **Read all documents** from collection (without embeddings)
2. **Generate new embeddings** with new model
3. **Delete old collection**
4. **Create new collection** with same name
5. **Add documents** with new embeddings

**Script**:
```python
# 1. Read documents
old_collection = client.get_collection("faultmaven_kb")
docs = old_collection.get(include=["documents", "metadatas"])

# 2. Generate new embeddings
from new_embedding_model import generate_embeddings
new_embeddings = generate_embeddings(docs["documents"])

# 3. Delete old, create new
client.delete_collection("faultmaven_kb")
new_collection = client.create_collection("faultmaven_kb")

# 4. Add with new embeddings
new_collection.add(
    ids=docs["ids"],
    documents=docs["documents"],
    metadatas=docs["metadatas"],
    embeddings=new_embeddings
)
```

**Downtime**: ~1-5 minutes for Global KB (depending on size)

---

### Monitoring & Observability

**Key Metrics to Track**:

| Metric | Description | Alert Threshold |
|--------|-------------|----------------|
| `chromadb.collection.count` | Number of documents per collection | N/A (trend) |
| `chromadb.query.latency_ms` | Query latency (p50, p95, p99) | p95 > 200ms |
| `chromadb.embedding.latency_ms` | Embedding generation time | p95 > 300ms |
| `chromadb.connection.errors` | Failed connection attempts | > 5 in 5min |
| `case_collection.created_count` | New case collections per hour | N/A (trend) |
| `case_collection.deleted_count` | Case collections cleaned up | N/A (trend) |
| `kb.ingestion.duration_ms` | Document ingestion time | p95 > 30s |

**Logging**:
- **INFO**: Collection creation/deletion, ingestion completion
- **ERROR**: Connection failures, embedding generation errors, search timeouts
- **DEBUG**: Individual query performance, chunk counts

**Grafana Dashboard** (Recommended):
- Query latency over time (per KB type)
- Collection size growth
- Ingestion throughput (docs/hour)
- Error rate

---

## Configuration Reference

### Embedding Model Configuration

**Current** (Hardcoded in code):
```python
# In KnowledgeIngester.__init__()
self.embedding_model = model_cache.get_bge_m3_model()
```

**Future** (Configurable via settings):
```bash
# In .env
EMBEDDING_MODEL=BAAI/bge-m3           # Current default
# EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2  # Alternative
```

**Switching Models**:
- Requires reindexing all collections (see "Reindexing" above)
- No hot-swap capability (all collections must use same model)

---

### Chunking Parameters

**Global KB** (Semantic chunking):
```python
chunk_size = 512  # tokens
overlap = 128     # tokens
```

**Case Evidence** (Type-specific):
- **Logs**: Variable (crime scene-based, ~50-200 lines)
- **Metrics**: Fixed (per time window, e.g., 1-hour buckets)
- **Config**: Variable (section-based)
- **Code**: Variable (function/class-based)
- **Text**: 512 tokens, 128 overlap

**Tuning Recommendations**:
- Larger chunks (1024 tokens): Better context, but less precise retrieval
- Smaller chunks (256 tokens): More precise, but may lose context
- **Current default (512)**: Good balance for most use cases

---

### Collection Metadata Schemas

**Global KB**:
```python
{
    "document_id": str,      # UUID
    "title": str,            # Document title
    "document_type": str,    # "troubleshooting_guide" | "error_reference" | "architecture_doc"
    "tags": List[str],       # Categorization tags
    "source_url": str,       # Optional source URL
    "ingested_at": str,      # ISO8601 timestamp
    "chunk_index": int,      # 0-based chunk number
    "total_chunks": int      # Total chunks for this document
}
```

**User KB** (planned):
```python
{
    "document_id": str,
    "user_id": str,              # Owning user
    "document_title": str,
    "category": str,             # "runbooks" | "procedures" | "reference"
    "tags": List[str],
    "uploaded_at": str,
    "last_modified": str,
    "chunk_index": int,
    "total_chunks": int
}
```

**Case Evidence**:
```python
{
    "case_id": str,
    "document_id": str,
    "filename": str,
    "file_type": str,            # "logs_and_errors" | "metrics" | "config" | "code" | "text"
    "uploaded_at": str,
    "file_size_bytes": int,
    "chunk_index": int,
    "total_chunks": int,

    # Type-specific metadata (optional)
    "line_number": int,          # For logs/code
    "timestamp": str,            # For logs/metrics
    "severity": str,             # For logs
    "metric_name": str,          # For metrics
    "metric_value": float,       # For metrics
    "section_path": str          # For config
}
```

---

### Performance Parameters

**Vector Search**:
```python
k = 5                    # Number of results to retrieve
                         # Tuning: 3-10 depending on use case
                         # More results = better coverage, but slower LLM synthesis
```

**Similarity Threshold** (Optional filtering):
```python
# Currently not used, but could filter low-quality matches
min_similarity = 0.5     # Cosine similarity threshold
                         # Only return results with score > 0.5
```

**Batch Size** (Embedding generation):
```python
batch_size = 32          # Embeddings per batch
                         # Larger = faster, but more memory
                         # BGE-M3 handles 32 well on most hardware
```

---

## Future Improvements

### 1. Hybrid Search Pipeline

**Current**: Pure vector similarity search

**Enhancement**: Three-stage hybrid retrieval for better precision

```python
# Stage 1: Metadata Pre-filtering (ChromaDB supports this today)
where_clause = {
    "severity": "CRITICAL",
    "timestamp": {"$gte": "2025-10-21T00:00:00Z", "$lt": "2025-10-22T00:00:00Z"}
}

candidates = collection.query(
    query_embedding=embedding,
    where=where_clause,  # Filter by metadata BEFORE vector search
    n_results=k * 3      # Oversample for re-ranking
)

# Stage 2: Vector Similarity (semantic search on pre-filtered results)
# Already filtered by metadata, more efficient

# Stage 3: Hybrid Re-ranking (combine vector + keyword scores)
def hybrid_rerank(query: str, candidates: List, Î±=0.7):
    """
    Hybrid score = Î± Ã— vector_score + (1-Î±) Ã— bm25_score

    Args:
        Î±: Weight for vector score (0.7 = 70% semantic, 30% lexical)
    """
    reranked = []
    for candidate in candidates:
        vector_score = candidate['distance']  # Already computed
        keyword_score = bm25_score(query, candidate['document'])  # Compute now

        hybrid_score = Î± * vector_score + (1 - Î±) * keyword_score
        reranked.append((candidate, hybrid_score))

    # Return top-k by hybrid score
    reranked.sort(key=lambda x: x[1], reverse=True)
    return [item[0] for item in reranked[:k]]
```

**Benefits**:
- **+30-50% precision** for queries with specific criteria
- **+90% accuracy** for exact matches (line numbers, error codes, timestamps)
- **-10-20% latency** (metadata filtering reduces vector search space)

**Implementation**:
- Metadata filtering: Already supported by ChromaDB `where` parameter
- BM25 scoring: Lightweight library (rank-bm25 or custom implementation)
- No breaking changes to API or agent interface

### 2. Conversation-Aware Query Enhancement

**Current**: Tools treat every query independently

**Enhancement**: Optional conversation context parameter for query rewriting

**API Enhancement**:
```python
# Current API
POST /api/v1/knowledge/query
{
    "kb_type": "case_evidence",
    "case_id": "abc123",
    "question": "When did they start?",  # Ambiguous!
    "k": 5
}

# Enhanced API (backward compatible)
POST /api/v1/knowledge/query
{
    "kb_type": "case_evidence",
    "case_id": "abc123",
    "question": "When did they start?",
    "k": 5,
    "conversation_context": [           # NEW: Optional
        {"role": "user", "content": "What errors in the log?"},
        {"role": "assistant", "content": "347 connection timeout errors"}
    ]
}
```

**Internal Query Enhancement**:
```python
# Tool rewrites ambiguous query using context
# "When did they start?" â†’ "When did the connection timeout errors start?"

# Then proceeds with normal retrieval pipeline
enhanced_query = rewrite_query(question, conversation_context)
embedding = embed(enhanced_query)  # Better embedding!
results = collection.query(query_embedding=embedding, n_results=k)
```

**Benefits**:
- **+40-60% success rate** for follow-up queries
- **+95% pronoun resolution** accuracy
- **No breaking changes** (conversation_context is optional)

**Implementation**:
- Query rewriting: One GPT-4-mini call (negligible cost: $0.0001/query)
- Backward compatible (optional parameter)
- Graceful fallback if rewriting fails

### Implementation Priority

1. **Hybrid Search First**: Highest ROI, already supported by ChromaDB, low risk
2. **Conversation-Aware Second**: High impact for investigations, requires conversation context plumbing

Both enhancements maintain:
- âœ… Pure function design (tools remain `(question, context?) â†’ answer`)
- âœ… Agent-managed context architecture
- âœ… Offline ingestion, live retrieval separation

See [qa-tools-design.md](./qa-tools-design.md) "Future Enhancements" section for detailed design.

---

## Related Documents

### Core Knowledge Base Documentation
- [knowledge-base-architecture.md](./knowledge-base-architecture.md) - **Storage layer**: Three KB systems, Strategy Pattern, ChromaDB collections, offline ingestion
- [qa-tools-design.md](./qa-tools-design.md) - **Access layer**: Stateless Q&A tools, prompt engineering, main agent tool selection
- [vector-database-operations.md](./vector-database-operations.md) (this document) - **Operations**: Ingestion pipelines, query flows, lifecycle management, admin procedures

### Supporting Documentation
- [data-preprocessing-design-specification.md](./data-preprocessing-design-specification.md) - Preprocessing pipeline details (classification, extraction, chunking)
- [HANDLING_LARGE_DOCUMENTS.md](./HANDLING_LARGE_DOCUMENTS.md) - Chunking strategies for large files

---

## Summary

This document provides comprehensive operational details for FaultMaven's vector database system:

âœ… **Physical Architecture**: Single ChromaDB instance, multiple collections, BGE-M3 embeddings
âœ… **Collection Lifecycle**: Global (permanent), User (per-user permanent), Case (ephemeral)
âœ… **Ingestion Pipeline**: Classification â†’ Extraction â†’ Chunking â†’ Sanitization â†’ Embedding â†’ Storage
âœ… **Query Operations**: Embedding â†’ Vector Search â†’ Top-K â†’ LLM Synthesis
âœ… **Updates/Deletes**: Hard replacement for documents, collection deletion for cases
âœ… **Operational Procedures**: Admin workflows, backup/restore, reindexing, monitoring
âœ… **Configuration**: Embedding models, chunking parameters, metadata schemas, performance tuning

For implementation details of the Strategy Pattern and KB-neutral design, see [knowledge-base-architecture.md](./knowledge-base-architecture.md).

For prompt engineering and access layer design, see [qa-tools-design.md](./qa-tools-design.md).
