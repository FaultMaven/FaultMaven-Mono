# FaultMaven Environment Configuration
# Copy this file to .env and edit with your values

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Primary chat provider (openai, anthropic, fireworks)
CHAT_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Fireworks AI Configuration
FIREWORKS_API_KEY=your_fireworks_api_key_here
FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct

# Local LLM Configuration (for self-hosted models)
LOCAL_LLM_MODEL=llama2-7b                           # Model name (directory in /var/lib/llm/)
LOCAL_LLM_BASE_URL=http://localhost:5000            # Local LLM server endpoint

# LLM Provider Behavior
STRICT_PROVIDER_MODE=false                          # If true, disable fallback to other providers

# =============================================================================
# Service Connections (K8s Cluster - Hybrid Ingress + NodePort)
# =============================================================================

# Redis Session Storage (NodePort - TCP binary protocol requires direct access)
REDIS_HOST=192.168.0.111
REDIS_PORT=30379
REDIS_PASSWORD=faultmaven-dev-redis-2025

# Session Timeout Configuration (for frontend crash recovery)
SESSION_MIN_TIMEOUT_MINUTES=60          # Minimum session timeout (1 hour)
SESSION_MAX_TIMEOUT_MINUTES=480         # Maximum session timeout (8 hours)
SESSION_DEFAULT_TIMEOUT_MINUTES=180     # Default session timeout (3 hours)
SESSION_CLEANUP_INTERVAL_MINUTES=30     # How often to run cleanup (30 minutes)

# Knowledge Base (ChromaDB)
CHROMADB_URL=http://chromadb.faultmaven.local:30080
CHROMADB_API_KEY=your_chromadb_token_here

# PII Protection (Presidio)
PRESIDIO_ANALYZER_URL=http://presidio-analyzer.faultmaven.local:30080
PRESIDIO_ANONYMIZER_URL=http://presidio-anonymizer.faultmaven.local:30080

# Observability (Opik)
OPIK_USE_LOCAL=true
OPIK_LOCAL_URL=http://opik.faultmaven.local:30080
OPIK_LOCAL_HOST=opik.faultmaven.local
OPIK_PROJECT_NAME=FaultMaven Development
OPIK_API_KEY=local-dev-key

# =============================================================================
# Application Configuration
# =============================================================================

# Logging Level
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_DEDUPE=true
LOG_BUFFER_SIZE=100
LOG_FLUSH_INTERVAL=5

# Session Configuration
SESSION_TIMEOUT_MINUTES=30

# Development Flags
ENABLE_MIGRATION_LOGGING=false
SKIP_SERVICE_CHECKS=false

# Feature Flags (Phase 0 Task 1 - Intelligent Prompt System)

# Feature Flags (Enhanced Query Classification System - Phase 0)
# LLM Classification Mode: disabled, fallback, enhancement (recommended), always
LLM_CLASSIFICATION_MODE=enhancement                    # Enhancement mode: call LLM only when pattern confidence < threshold
PATTERN_CONFIDENCE_THRESHOLD=0.7                       # Confidence threshold for triggering LLM (0.0-1.0, 0.7 recommended)
ENABLE_MULTIDIMENSIONAL_CONFIDENCE=true                # Enable multi-dimensional confidence scoring (5 factors)
CONFIDENCE_OVERRIDE_THRESHOLD=0.4                      # Force clarification below this confidence (0.0-1.0, 0.4 recommended)
SELF_CORRECTION_MIN_CONFIDENCE=0.4                     # Lower bound for self-correction prompt (0.4-0.7 range recommended)
SELF_CORRECTION_MAX_CONFIDENCE=0.7                     # Upper bound for self-correction prompt

# Pattern Matching Configuration (Phase 0 - Weighted Patterns & Exclusions)
PATTERN_WEIGHTED_SCORING=true                          # Enable weighted pattern scores (0.5-2.0 specificity)
PATTERN_EXCLUSION_RULES=true                           # Enable exclusion patterns to prevent false positives

# Multi-Dimensional Confidence Analysis (Phase 0 - 5 Factors)
ENABLE_STRUCTURE_ANALYSIS=true                         # Analyze query structure (question vs statement, length)
ENABLE_LINGUISTIC_ANALYSIS=true                        # Analyze tense, certainty markers, linguistic patterns
ENABLE_ENTITY_ANALYSIS=true                            # Detect technical entities (error codes, versions, paths)
ENABLE_CONTEXT_ANALYSIS=true                           # Use conversation history for confidence boost
ENABLE_DISAMBIGUATION_CHECK=true                       # Check for cross-intent conflicts (penalty for ambiguity)

# Prompt Optimization (Phase 0 - Token Reduction 81%)

# Self-Correction Thresholds (Phase 0 - renamed for clarity)
SELF_CORRECTION_THRESHOLD=0.7                          # Confidence threshold for self-correction (same as max)
FORCED_CLARIFICATION_THRESHOLD=0.4                     # Force clarification below this (same as override)

# Legacy flag for backward compatibility (deprecated - use LLM_CLASSIFICATION_MODE instead)
ENABLE_LLM_CLASSIFICATION=true                         # Old flag: true = "always", false = "disabled"

# Feature Flags (Phase 0 Task 3 - Token-Aware Context Management)
ENABLE_TOKEN_AWARE_CONTEXT=true
ENABLE_CONVERSATION_SUMMARIZATION=true

# Conversation and Context Thresholds (centralized in ConversationThresholds)
# These control conversation flow, token budgets, and classification behavior
MAX_CLARIFICATIONS=3                               # Max clarification requests before suggesting escalation
MAX_CONVERSATION_TURNS=20                          # Max conversation turns to track in memory
MAX_CONVERSATION_TOKENS=4000                       # Max tokens for conversation history

# Token Budget Allocation (for prompt assembly optimization)
CONTEXT_TOKEN_BUDGET=4000                          # Total budget for system+user+history (default: 4000)

# Note: PATTERN_CONFIDENCE_THRESHOLD and other classification thresholds are above in classification section

# =============================================================================
# Protection System Configuration (Phase 1 Client Protection)
# =============================================================================

# General Protection Settings
PROTECTION_ENABLED=true
PROTECTION_FAIL_OPEN=true
PROTECTION_BYPASS_HEADERS=X-Dev-Bypass,X-Test-Bypass
ENVIRONMENT=development

# Rate Limiting Configuration
RATE_LIMITING_ENABLED=true

# Rate Limits (format: requests:window_seconds)
RATE_LIMIT_GLOBAL=1000:60
RATE_LIMIT_PER_SESSION=10:60
RATE_LIMIT_PER_SESSION_HOURLY=100:3600
RATE_LIMIT_TITLE_GENERATION=1:300

# Request Deduplication Configuration
DEDUPLICATION_ENABLED=true
DEDUP_DEFAULT_TTL=300
DEDUP_AGENT_QUERY_TTL=60

# Timeout Protection Configuration
TIMEOUTS_ENABLED=true
TIMEOUT_AGENT_TOTAL=300
TIMEOUT_AGENT_PHASE=120
TIMEOUT_LLM_CALL=30
TIMEOUT_EMERGENCY_SHUTDOWN=600

# Redis Configuration (for protection systems)
REDIS_KEY_PREFIX=faultmaven

# =============================================================================
# Protection System Configuration (Phase 2 Intelligent Protection)
# =============================================================================

# Phase 2 Master Toggle
PROTECTION_PHASE_1_ENABLED=true
PROTECTION_PHASE_2_ENABLED=true

# Behavioral Analysis
BEHAVIORAL_ANALYSIS_ENABLED=true
BEHAVIOR_ANALYSIS_WINDOW=3600
BEHAVIOR_PATTERN_THRESHOLD=0.8

# ML Anomaly Detection
ML_ANOMALY_DETECTION_ENABLED=true
ML_MODEL_PATH=/tmp/faultmaven_ml_models
ML_TRAINING_ENABLED=true
ML_ONLINE_LEARNING_ENABLED=true

# Reputation System
REPUTATION_SYSTEM_ENABLED=true
REPUTATION_DECAY_RATE=0.05
REPUTATION_RECOVERY_THRESHOLD=0.1

# Smart Circuit Breakers
SMART_CIRCUIT_BREAKERS_ENABLED=true
CIRCUIT_FAILURE_THRESHOLD=5
CIRCUIT_TIMEOUT_SECONDS=60

# System Monitoring
PROTECTION_MONITORING_INTERVAL=300
PROTECTION_CLEANUP_INTERVAL=3600

# =============================================================================
# Advanced Configuration
# =============================================================================

# Alternative: Use local services for development
# Uncomment these if running services locally instead of K8s cluster
# REDIS_URL=redis://localhost:6379
# CHROMADB_URL=http://localhost:8000
# PRESIDIO_ANALYZER_URL=http://localhost:5001
# PRESIDIO_ANONYMIZER_URL=http://localhost:5002

# Alternative: Use Docker Compose services
# Uncomment these if using docker-compose.yml
# REDIS_HOST=localhost
# REDIS_PORT=6379
# CHROMADB_URL=http://localhost:8000
# =============================================================================
# Targeted Tracing Configuration
# =============================================================================

# Global tracing control - set to 'true' to disable all tracing
OPIK_TRACK_DISABLE=false

# Target specific users for tracing (comma-separated list)
# Example: OPIK_TRACK_USERS=debug_user,test_user,admin@company.com
OPIK_TRACK_USERS=

# Target specific sessions for tracing (comma-separated list)
# Example: OPIK_TRACK_SESSIONS=session-abc-123,session-def-456
OPIK_TRACK_SESSIONS=

# Target specific operations for tracing (comma-separated list)
# Example: OPIK_TRACK_OPERATIONS=llm_query,agent_run,knowledge_search
OPIK_TRACK_OPERATIONS=

# =============================================================================
# Targeted Tracing Usage Examples
# =============================================================================
#
# 1. Debug specific user issues:
#    OPIK_TRACK_USERS=problematic_user_123
#
# 2. Monitor expensive operations only:
#    OPIK_TRACK_OPERATIONS=llm_query,vector_search,agent_run
#
# 3. Investigate specific session problems:
#    OPIK_TRACK_SESSIONS=session_with_errors_xyz
#
# 4. Combined targeting (user AND operation must match):
#    OPIK_TRACK_USERS=debug_user,qa_user
#    OPIK_TRACK_OPERATIONS=troubleshoot,generate_solution
#
# 5. Temporarily disable all tracing:
#    OPIK_TRACK_DISABLE=true
#
# Note: All targeting can be changed at runtime without restarting the server

# =============================================================================
# DOCTOR/PATIENT PROMPT SYSTEM
# =============================================================================
# Controls which prompt version is used for the revolutionary doctor/patient
# architecture. Different versions offer different trade-offs between token
# usage, latency, and guidance quality.
#
# Available versions:
# - minimal: ~800 tokens - Fast, cost-efficient, simple queries
# - standard: ~1,300 tokens - Balanced, recommended default (RECOMMENDED)
# - detailed: ~1,800 tokens - Maximum guidance, complex cases

# Prompt version selection (minimal/standard/detailed)
DOCTOR_PATIENT_PROMPT_VERSION=standard

# Future: Enable automatic version selection based on query complexity
ENABLE_DYNAMIC_PROMPT_VERSION=false

# When dynamic selection enabled:
# Query tokens below this use minimal prompt
MINIMAL_PROMPT_THRESHOLD=50

# Query complexity above this uses detailed prompt  
DETAILED_PROMPT_THRESHOLD=0.7
