# app/log_metrics_analysis.py
"""
Performs specialized analysis on different data types, attempting to make
LLM-based analysis context-aware based on provided conversation history.

Uses external 'vector' tool for log parsing. Contains placeholders for
metric, config, problem statement, and source code processing.
"""

import subprocess
import json
import re
from collections import defaultdict
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import statistics
import os
import asyncio
import time # Import time for logging/timestamps

from app.logger import logger
from config.settings import settings # For VECTOR_TIMEOUT and analysis thresholds
# Import models from the central models file
from app.models import LogInsights, DataType # LogInsights defines the output structure
# Import the shared LLM instance
from app.llm_provider import llm
# Import LangChain components
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers.string import StrOutputParser
# Import BaseMessage for type hinting, Runnables for chain modification
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnablePassthrough, RunnableLambda


# --- LLM Chains Specific to this Module (MODIFIED for Context Awareness) ---

# Chain for summarizing structured log insights generated by analyze_logs
log_summary_prompt_template = ChatPromptTemplate.from_messages([
    ("system", "Summarize the following structured log analysis data concisely, focusing on key observations, trends, anomalies, and potential issues relevant to the preceding conversation context. Limit the summary to 150 words."),
    MessagesPlaceholder(variable_name="history", optional=True),
    ("human", "Log Analysis Data:\n```\n{log_analysis_data}\n```\n\nConcise Summary:")
])
# Chain definition remains the same conceptually
log_summary_chain = (
    RunnablePassthrough.assign(history=RunnableLambda(lambda x: x.get("history", [])))
    | log_summary_prompt_template
    | llm
    | StrOutputParser()
)

# Chain for analyzing generic text data or problem statements
text_analysis_prompt_template = ChatPromptTemplate.from_messages([
     ("system", "Analyze the following text based on the preceding conversation context..."), # Keep your system prompt
     MessagesPlaceholder(variable_name="history", optional=True),
     ("human", "Text to Analyze:\n```\n{text_data}\n```\n\nAnalysis:")
])
# Chain definition remains the same conceptually
text_analysis_chain = (
     RunnablePassthrough.assign(history=RunnableLambda(lambda x: x.get("history", [])))
    | text_analysis_prompt_template
    | llm
    | StrOutputParser()
)


# --- Main Data Processing Router (MODIFIED Signature) ---
async def process_data(
    data: str,
    data_type: DataType,
    history: Optional[List[BaseMessage]] = None # Added history parameter
    ) -> Union[LogInsights, Dict[str, Any]]:
    """
    Routes incoming data to the appropriate specialized processing function,
    passing along conversation history for context.

    Args:
        data: The raw data content (string).
        data_type: The classified DataType enum member.
        history: Optional list of BaseMessage objects from the conversation.

    Returns:
        A LogInsights object for logs, or a dictionary for other types/errors.
    """
    logger.info(f"Processing data classified as type: {data_type.value} (History provided: {history is not None})")
    start_time = time.time()
    result: Union[LogInsights, Dict[str, Any]] # Define type hint for result

    # Route based on type, passing history down
    if data_type == DataType.SYSTEM_LOGS:
        result = await process_logs_data(data, history=history)
    elif data_type == DataType.MONITORING_METRICS:
        result = process_metrics_data(data, history=history) # Pass history to placeholder
    elif data_type == DataType.CONFIGURATION_DATA:
        result = process_config_data(data, history=history) # Pass history to placeholder
    elif data_type == DataType.ISSUE_DESCRIPTION:
        logger.info(f"Processing Problem Statement via text analysis...")
        result = await process_text_data(data, history=history) # Use text analysis with history
    elif data_type == DataType.SOURCE_CODE:
        logger.info(f"Triggering source code processing...")
        # result = await process_source_code(data, history=history) # Call when implemented
        result = {"message": "Source code processing not yet implemented."} # Placeholder
    # Note: MCP type removed from enum
    elif data_type == DataType.TEXT:
        result = await process_text_data(data, history=history) # Use text analysis with history
    else: # Handle UNKNOWN
        logger.warning(f"No specific processing function for data type: {data_type.value}")
        result = {"error": f"Unsupported data type for specialized processing: {data_type.value}"}

    end_time = time.time()
    logger.info(f"Finished processing data type {data_type.value} in {end_time - start_time:.2f}s")
    return result


# --- Log Processing Function (MODIFIED Signature) ---
async def process_logs_data(
    data: str,
    history: Optional[List[BaseMessage]] = None # Added history parameter
    ) -> LogInsights:
    """
    Processes raw log data using Vector, performs analysis, and generates
    a potentially context-aware LLM summary (asynchronous).

    Args:
        data: Raw log data as a string.
        history: Optional list of history messages for contextual summary.

    Returns:
        A LogInsights object containing structured analysis and summary.
    """
    if not data:
         logger.warning("process_logs_data called with empty data.")
         return LogInsights(summary="No log data provided.")

    try:
        # --- 1. Run Vector Subprocess ---
        command = ["vector", "--config", "app/vector.yaml"]
        timeout = settings.VECTOR_TIMEOUT
        logger.info(f"Running log processing via Vector (Timeout: {timeout}s)...")
        process = await asyncio.to_thread(
             subprocess.run,
             command,
             input=data.encode("utf-8"),
             capture_output=True,
             check=True,
             timeout=timeout,
         )
        vector_output = process.stdout.decode("utf-8")
        logger.info(f"Vector process completed successfully ({len(vector_output)} bytes output).")
        logger.debug(f"Vector raw output snippet:\n{vector_output[:500]}...")

        # --- 2. Parse Vector Output ---
        parsed_logs: List[Dict[str, Any]] = []
        for line in vector_output.strip().splitlines():
            if line:
                try:
                    parsed_logs.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.warning(f"Skipping line due to JSON decode error: {e}. Line: '{line[:100]}...'")
                    continue
        if not parsed_logs:
             logger.warning("Vector output contained no parsable JSON log entries.")
             return LogInsights(summary="Log processing yielded no structured entries.")
        logger.info(f"Parsed {len(parsed_logs)} log entries from Vector output.")

        # --- 3. Analyze Parsed Logs (Synchronous - No History Needed for this part) ---
        insights_dict = analyze_logs(parsed_logs)
        logger.info("Log analysis completed.")

        # --- 4. Generate Context-Aware LLM Summary ---
        insights = LogInsights(**insights_dict)
        logger.info(f"Generating LLM summary for log insights (history provided: {history is not None})...")
        # Pass history down to the summary function
        insights.summary = await process_data_summary(insights, history=history)
        logger.info("LLM log summary generated.")

        return insights

    # --- Error Handling for Subprocess (Ensure 'as e' added previously) ---
    except subprocess.CalledProcessError as e:
        stderr_output = e.stderr.decode(errors='ignore') if e.stderr else "No stderr output."
        logger.error(f"Vector process failed! Exit Code: {e.returncode}. Stderr: {stderr_output}")
        raise ValueError(f"Log processing via Vector failed: {stderr_output}") from e
    except subprocess.TimeoutExpired as e:
        logger.error(f"Vector process timed out after {timeout} seconds.")
        raise ValueError(f"Log processing via Vector timed out ({timeout}s).") from e
    except FileNotFoundError as e:
        logger.error("Vector command not found. Ensure 'vector' is installed and in the system PATH.")
        raise RuntimeError("Required tool 'vector' not found.") from e
    except Exception as e:
        logger.exception(f"An unexpected error occurred during log processing: {e}")
        if isinstance(e, ValueError): raise # Re-raise ValueErrors (e.g., from JSON parsing inside)
        raise RuntimeError(f"Unexpected error during log processing: {e}") from e


# --- Log Analysis Function (Synchronous - No Change) ---
def analyze_logs(parsed_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Performs basic statistical analysis on parsed log data.
    This part does not require conversation history.
    """
    # ... (Implementation remains the same as Response #91 - Corrected anomaly loop) ...
    level_counts: Dict[str, int] = defaultdict(int)
    error_messages: List[str] = []
    anomalies: List[str] = []
    metrics_values: Dict[str, List[float]] = defaultdict(list)

    for log_entry in parsed_logs:
        if not isinstance(log_entry, dict): continue
        level = log_entry.get("level", log_entry.get("severity", "unknown")).lower()
        level_counts[level] += 1
        message = log_entry.get("message", "")
        if level in ["error", "err", "critical", "fatal", "severe"] and message:
              error_messages.append(message)
        resp_time = log_entry.get("response_time_ms") or log_entry.get("duration_ms")
        if resp_time is not None:
            try: metrics_values["response_time_ms"].append(float(resp_time))
            except (ValueError, TypeError): logger.debug(f"Could not convert metric 'response_time_ms' to float: {resp_time}")
        status_code = log_entry.get("status_code", log_entry.get("status"))
        if status_code is not None:
             try: metrics_values["status_code"].append(int(status_code))
             except (ValueError, TypeError): logger.debug(f"Could not convert metric 'status_code' to int: {status_code}")

    rt_values = metrics_values.get("response_time_ms", [])
    if len(rt_values) >= settings.min_data_points_for_anomaly_detection:
        try:
            mean_rt = statistics.mean(rt_values)
            stdev_rt = statistics.stdev(rt_values)
            threshold = mean_rt + settings.metric_anomaly_threshold_std_dev * stdev_rt
            for rt in rt_values: # Iterate collected values
                if rt > threshold:
                    anomalies.append(f"High response time detected: {rt:.2f}ms (mean: {mean_rt:.2f}, threshold: {threshold:.2f})")
        except statistics.StatisticsError as stat_err:
             logger.warning(f"Could not calculate statistics for response time anomaly detection: {stat_err}")
        except Exception as e:
             logger.error(f"Unexpected error during response time anomaly detection: {e}", exc_info=True)

    total_logs = len(parsed_logs)
    error_count = sum(level_counts.get(lvl, 0) for lvl in ["error", "err", "critical", "fatal", "severe"])
    if total_logs > 10 and error_count / total_logs > 0.1:
        anomalies.append(f"High error ratio detected: {error_count}/{total_logs} ({error_count/total_logs:.1%})")

    averaged_metrics = {}
    for key, values in metrics_values.items():
      if values:
          try:
              if key == "status_code":
                   status_dist = defaultdict(int)
                   for sc in values: status_dist[sc] += 1
                   averaged_metrics[key + "_distribution"] = dict(sorted(status_dist.items()))
              else:
                   averaged_metrics[key] = statistics.mean(values)
          except statistics.StatisticsError: averaged_metrics[key] = "Stats Calculation Error"
          except Exception as e: averaged_metrics[key] = "Calculation Error"

    return {
        "level_counts": dict(level_counts),
        "error_messages": error_messages[:20],
        "anomalies": anomalies[:10],
        "metrics": averaged_metrics,
        "summary": "" # Placeholder for LLM summary
    }


# --- LLM Data Summary Function (MODIFIED Signature & Call) ---
async def process_data_summary(
    log_insights: LogInsights,
    history: Optional[List[BaseMessage]] = None # Added history parameter
    ) -> str:
    """Generates a potentially context-aware LLM summary of log analysis results."""
    try:
        log_data_string = format_log_data_for_summary(log_insights)
        if not log_data_string.strip():
             logger.warning("No analysis data to summarize for LLM.")
             return "No specific insights were extracted programmatically from the logs."

        # --- Prepare input dictionary for the context-aware chain ---
        chain_input = {
            "log_analysis_data": log_data_string,
            "history": history or [] # Pass history (or empty list) to the chain
        }
        logger.debug(f"Invoking log_summary_chain (history length: {len(history or [])})...")
        # --- Invoke the updated chain ---
        result = await log_summary_chain.ainvoke(chain_input)

        if result and isinstance(result, str) and result.strip():
            summary = result.strip()
            logger.debug(f"LLM log summary generated: '{summary[:100]}...'")
            return summary
        else:
            logger.error(f"LLM returned empty/invalid response for data summary: {result}")
            return "Error: LLM returned no summary."
    except Exception as e:
        logger.exception(f"LLM data summary generation failed: {e}")
        return "Error: Failed to generate summary via LLM."


# --- Formatting Helper (Synchronous - No Change Needed) ---
# Ensure this is the corrected version that returns default message
def format_log_data_for_summary(log_insights: LogInsights) -> str:
    """Formats LogInsights data into a string suitable for LLM summary prompt."""
    # ... (Implementation from Response #117 - includes summary field check and default msg) ...
    parts = []
    if log_insights.level_counts: parts.append(f"- Log Levels Found: {', '.join(f'{k}: {v}' for k, v in log_insights.level_counts.items())}")
    if log_insights.metrics: parts.append(f"- Derived Metrics: {', '.join(f'{k}: {v:.2f}' for k, v in log_insights.metrics.items())}")
    if log_insights.anomalies: parts.append("- Detected Anomalies/Patterns:"); parts.extend([f"  - {a}" for a in log_insights.anomalies[:5]]);
    if len(log_insights.anomalies) > 5: parts.append("  - ... (more anomalies found)")
    if log_insights.summary and log_insights.summary.strip() and "Error generating" not in log_insights.summary: parts.append(f"- Processing Summary: {log_insights.summary}")
    if log_insights.error_messages: parts.append("- Sample Error Messages:"); parts.extend([f"  - {e[:150]}{'...' if len(e) > 150 else ''}" for e in log_insights.error_messages[:3]]);
    if len(log_insights.error_messages) > 3: parts.append("  - ... (more errors found)")
    return "\n".join(parts) if parts else "Log data processed, but no specific programmatic insights extracted."


# --- Placeholder Functions (MODIFIED Signatures) ---
def process_metrics_data(data: str, history: Optional[List[BaseMessage]] = None) -> Dict[str, Any]:
    """Placeholder for processing metrics data (now accepts history)."""
    logger.warning("Metrics processing not yet implemented.")
    # TODO: Implement metrics parsing/analysis, potentially using history
    return {"message": "Metrics data processing is not yet implemented.", "data_snippet": data[:200]}

def process_config_data(data: str, history: Optional[List[BaseMessage]] = None) -> Dict[str, Any]:
    """Placeholder for processing configuration data (now accepts history)."""
    logger.warning("Config processing not yet implemented.")
    # TODO: Implement config parsing/analysis, potentially using history
    return {"message": "Configuration data processing is not yet implemented.", "data_snippet": data[:200]}

def process_problem_report_data(data: str, history: Optional[List[BaseMessage]] = None) -> Dict[str, Any]:
    """Placeholder for processing problem reports/RCAs (now accepts history)."""
    logger.warning("Problem report processing not yet implemented.")
    # TODO: Could route to process_text_data or implement specific logic
    return {"message": "Problem report/RCA processing is not yet implemented.", "data_snippet": data[:200]}


# --- Text Processing Function (MODIFIED Signature & Call) ---
async def process_text_data(
    data: str,
    history: Optional[List[BaseMessage]] = None # Added history parameter
    ) -> Dict[str, Any]:
    """Processes generic text or problem statements using LLM (now context-aware)."""
    if not data:
        logger.warning("process_text_data called with empty data.")
        return {"summary": "No text data provided."}
    try:
        # --- Prepare input for context-aware chain ---
        chain_input = {
            "text_data": data,
            "history": history or [] # Pass history (or empty list)
        }
        logger.debug(f"Invoking text_analysis_chain (history length: {len(history or [])})...")
        # --- Invoke the updated chain ---
        result = await text_analysis_chain.ainvoke(chain_input)

        if result and isinstance(result, str) and result.strip():
            summary = result.strip()
            logger.debug(f"LLM text analysis generated: '{summary[:100]}...'")
            return {"summary": summary}
        else:
            logger.error(f"LLM returned empty/invalid response for text data: {result}")
            return {"error": "LLM returned no analysis for the text."}
    except Exception as e:
        logger.exception(f"LLM text processing failed: {e}")
        return {"error": "Error processing text data with LLM."}