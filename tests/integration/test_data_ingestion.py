"""
Test 2: Data Ingestion Pipeline

Objective: To verify that submitting log data through the /data endpoint
triggers the full classification and processing pipeline, and that the
resulting insights are correctly stored in the user's session.

Setup: The backend API server and Redis container must be running.
"""

import io
import json
from typing import Any, Dict

import httpx
import pytest
import redis.asyncio as redis


@pytest.mark.asyncio
async def test_data_ingestion_pipeline(
    http_client: httpx.AsyncClient,
    redis_client: redis.Redis,
    test_session: Dict[str, Any],
    sample_log_content: str,
    clean_redis: None,
):
    """
    Test Steps:
    1. Create a new session to get a session_id
    2. Read sample log content from fixture
    3. Send a POST request to /data endpoint with multipart/form-data
    4. Assert HTTP response is 200 OK with valid DataInsightsResponse
    5. Connect to Redis and fetch the session data
    6. Assert session data contains insights generated by LogProcessor
    """
    session_id = test_session["session_id"]

    # Step 2: Prepare log file upload
    log_file = io.BytesIO(sample_log_content.encode("utf-8"))

    # Step 3: Send POST request to /data endpoint
    files = {"file": ("test.log", log_file, "text/plain")}
    data = {
        "session_id": session_id,
        "description": "Sample log file for testing data ingestion pipeline",
    }

    response = await http_client.post("/api/v1/data/", files=files, data=data)

    # Step 4: Assert HTTP response is 200 OK with valid DataInsightsResponse
    assert response.status_code == 200

    insights_response = response.json()

    # Verify required fields in DataInsightsResponse
    assert "data_id" in insights_response
    assert "data_type" in insights_response
    assert "insights" in insights_response
    assert "confidence_score" in insights_response
    assert "processing_time_ms" in insights_response
    assert "anomalies_detected" in insights_response
    assert "recommendations" in insights_response

    # Verify insights are not empty
    assert insights_response["insights"] is not None
    assert len(insights_response["insights"]) > 0

    # Verify data type classification
    assert insights_response["data_type"] == "log_file"

    # Verify confidence score is reasonable
    assert 0.0 <= insights_response["confidence_score"] <= 1.0

    # Verify anomalies and recommendations are lists
    assert isinstance(insights_response["anomalies_detected"], list)
    assert isinstance(insights_response["recommendations"], list)

    data_id = insights_response["data_id"]

    # Step 5: Connect to Redis and fetch the session data
    session_key = f"session:{session_id}"
    redis_session_data = await redis_client.get(session_key)
    assert redis_session_data is not None

    session_context = json.loads(redis_session_data)

    # Step 6: Assert session data contains insights generated by LogProcessor
    # Verify data_id was added to session uploads
    assert data_id in session_context["data_uploads"]

    # Verify investigation history was updated
    assert len(session_context["investigation_history"]) > 0

    # Find the data upload history entry
    data_upload_history = None
    for history_item in session_context["investigation_history"]:
        if (
            history_item.get("action") == "data_upload"
            and history_item.get("data_id") == data_id
        ):
            data_upload_history = history_item
            break

    assert data_upload_history is not None
    assert data_upload_history["data_type"] == "log_file"
    assert data_upload_history["file_name"] == "test.log"
    assert "insights" in data_upload_history
    assert data_upload_history["insights"] is not None


@pytest.mark.asyncio
async def test_data_ingestion_with_different_file_types(
    http_client: httpx.AsyncClient,
    redis_client: redis.Redis,
    test_session: Dict[str, Any],
    clean_redis: None,
):
    """
    Test data ingestion with different file types and content.
    """
    session_id = test_session["session_id"]

    # Test with different file types
    test_cases = [
        {
            "content": "ERROR: Database connection failed\nStack trace: ...",
            "filename": "error.log",
            "expected_type": "log_file",
        },
        {
            "content": "# Configuration File\nserver.port=8080\n" "db.host=localhost",
            "filename": "config.properties",
            "expected_type": "config_file",
        },
        {
            "content": "java.lang.NullPointerException\n"
            "\tat com.example.Service.method",
            "filename": "stack_trace.txt",
            "expected_type": "stack_trace",
        },
    ]

    for test_case in test_cases:
        content = test_case["content"]
        filename = test_case["filename"]

        # Upload file
        files = {"file": (filename, io.BytesIO(content.encode("utf-8")), "text/plain")}
        data = {"session_id": session_id, "description": f"Test file: {filename}"}

        response = await http_client.post("/api/v1/data/", files=files, data=data)
        assert response.status_code == 200

        insights_response = response.json()

        # Verify basic response structure
        assert "data_id" in insights_response
        assert "data_type" in insights_response
        assert "insights" in insights_response

        # Note: The expected_type depends on the classifier implementation
        # We just verify that classification occurred
        assert insights_response["data_type"] in [
            "log_file",
            "config_file",
            "stack_trace",
            "error_message",
            "documentation",
            "metrics_data",
            "unknown",
        ]


@pytest.mark.asyncio
async def test_data_ingestion_error_handling(
    http_client: httpx.AsyncClient, clean_redis: None
):
    """
    Test error handling in data ingestion pipeline.
    """
    # Test with invalid session_id
    files = {"file": ("test.log", io.BytesIO(b"test content"), "text/plain")}
    data = {"session_id": "invalid-session-id", "description": "Test file"}

    response = await http_client.post("/api/v1/data/", files=files, data=data)
    assert response.status_code == 404

    error_response = response.json()
    assert "detail" in error_response
    assert "Session not found" in error_response["detail"]


@pytest.mark.asyncio
async def test_data_ingestion_empty_file(
    http_client: httpx.AsyncClient, test_session: Dict[str, Any], clean_redis: None
):
    """
    Test data ingestion with empty file.
    """
    session_id = test_session["session_id"]

    # Upload empty file
    files = {"file": ("empty.log", io.BytesIO(b""), "text/plain")}
    data = {"session_id": session_id, "description": "Empty test file"}

    response = await http_client.post("/api/v1/data/", files=files, data=data)

    # The system should handle empty files gracefully
    assert response.status_code == 200

    insights_response = response.json()
    assert "data_id" in insights_response
    assert "data_type" in insights_response
    assert "insights" in insights_response


@pytest.mark.asyncio
async def test_data_ingestion_large_file(
    http_client: httpx.AsyncClient, test_session: Dict[str, Any], clean_redis: None
):
    """
    Test data ingestion with larger file content.
    """
    session_id = test_session["session_id"]

    # Create a larger log file content
    large_content = ""
    for i in range(100):
        large_content += (
            f"2024-01-15 14:30:{i:02d}.123 [ERROR] "
            f"Error #{i}: Connection timeout after 30 seconds\n"
        )
        large_content += f"    at Service.method{i}" f"(Service.java:{i+100})\n"
        large_content += f"    at Controller.handle{i}" f"(Controller.java:{i+200})\n"

    # Upload large file
    files = {
        "file": ("large.log", io.BytesIO(large_content.encode("utf-8")), "text/plain")
    }
    data = {"session_id": session_id, "description": "Large test log file"}

    response = await http_client.post("/api/v1/data/", files=files, data=data)
    assert response.status_code == 200

    insights_response = response.json()
    assert "data_id" in insights_response
    assert "data_type" in insights_response
    assert insights_response["data_type"] == "log_file"

    # Verify processing time is recorded
    assert "processing_time_ms" in insights_response
    assert insights_response["processing_time_ms"] >= 0


@pytest.mark.asyncio
async def test_multiple_data_uploads_same_session(
    http_client: httpx.AsyncClient,
    redis_client: redis.Redis,
    test_session: Dict[str, Any],
    sample_log_content: str,
    clean_redis: None,
):
    """
    Test multiple data uploads to the same session.
    """
    session_id = test_session["session_id"]

    # Upload multiple files to the same session
    data_ids = []
    for i in range(3):
        content = f"Log file {i}\n{sample_log_content}"

        files = {
            "file": (f"test_{i}.log", io.BytesIO(content.encode("utf-8")), "text/plain")
        }
        data = {"session_id": session_id, "description": f"Test file {i}"}

        response = await http_client.post("/api/v1/data/", files=files, data=data)
        assert response.status_code == 200

        insights_response = response.json()
        data_ids.append(insights_response["data_id"])

    # Verify all uploads are tracked in session
    session_key = f"session:{session_id}"
    redis_session_data = await redis_client.get(session_key)
    session_context = json.loads(redis_session_data)

    # All data IDs should be in the session
    for data_id in data_ids:
        assert data_id in session_context["data_uploads"]

    # Should have 3 upload history entries
    upload_history_count = sum(
        1
        for item in session_context["investigation_history"]
        if item.get("action") == "data_upload"
    )
    assert upload_history_count == 3


@pytest.mark.asyncio
async def test_data_retrieval_endpoint(
    http_client: httpx.AsyncClient,
    test_session: Dict[str, Any],
    sample_log_content: str,
    clean_redis: None,
):
    """
    Test retrieving data insights after upload.
    """
    session_id = test_session["session_id"]

    # Upload a file first
    files = {
        "file": (
            "test.log",
            io.BytesIO(sample_log_content.encode("utf-8")),
            "text/plain",
        )
    }
    data = {"session_id": session_id, "description": "Test file for retrieval"}

    upload_response = await http_client.post("/api/v1/data/", files=files, data=data)
    assert upload_response.status_code == 200

    upload_data = upload_response.json()
    data_id = upload_data["data_id"]

    # Retrieve the data insights
    response = await http_client.get(
        f"/api/v1/data/{data_id}", params={"session_id": session_id}
    )

    assert response.status_code == 200
    insights_response = response.json()

    # Verify response structure
    assert "data_id" in insights_response
    assert insights_response["data_id"] == data_id


@pytest.mark.asyncio
async def test_list_session_uploads(
    http_client: httpx.AsyncClient,
    test_session: Dict[str, Any],
    sample_log_content: str,
    clean_redis: None,
):
    """
    Test listing all uploads for a session.
    """
    session_id = test_session["session_id"]

    # Upload a couple of files
    for i in range(2):
        files = {
            "file": (
                f"test_{i}.log",
                io.BytesIO(sample_log_content.encode("utf-8")),
                "text/plain",
            )
        }
        data = {"session_id": session_id, "description": f"Test file {i}"}

        response = await http_client.post("/api/v1/data/", files=files, data=data)
        assert response.status_code == 200

    # List uploads for the session
    response = await http_client.get(f"/api/v1/data/session/{session_id}/uploads")
    assert response.status_code == 200

    uploads_response = response.json()
    assert "session_id" in uploads_response
    assert "uploads" in uploads_response
    assert "total_uploads" in uploads_response

    assert uploads_response["session_id"] == session_id
    assert uploads_response["total_uploads"] == 2
    assert len(uploads_response["uploads"]) == 2
